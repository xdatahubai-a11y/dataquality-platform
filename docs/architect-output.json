{
  "summary": "Architecture for DataQuality Platform: Python FastAPI monorepo with PySpark DQ engine, React dashboard, PostgreSQL metadata store, deployed on Azure Container Apps.",
  "techStack": {
    "backend": "Python 3.11 + FastAPI + SQLAlchemy 2.0 + Alembic",
    "frontend": "React 18 + TypeScript + Recharts + TailwindCSS",
    "database": "PostgreSQL 15",
    "spark": "PySpark 3.5 + Delta Lake 3.0",
    "cloud": "Azure Container Apps + ADLS Gen2 + Key Vault + Managed Identity"
  },
  "decisions": [
    {"topic": "Monorepo Structure", "decision": "Single repo with /api, /engine, /connectors, /spark, /dashboard, /infra, /tests", "rationale": "Simplifies development, CI/CD, and versioning for a small team", "alternatives": ["Multi-repo", "Python monorepo with npm workspace"]},
    {"topic": "ORM", "decision": "SQLAlchemy 2.0 with async support", "rationale": "Mature, well-documented, good FastAPI integration via async sessions", "alternatives": ["Tortoise ORM", "SQLModel", "Raw SQL"]},
    {"topic": "Job Queue", "decision": "In-process async for MVP, Redis/Service Bus for Phase 2", "rationale": "Reduces infrastructure complexity for MVP; Spark submit is the bottleneck, not the queue", "alternatives": ["Celery + Redis", "Azure Service Bus", "RQ"]},
    {"topic": "Auth", "decision": "API key auth for MVP, Azure AD for Phase 2", "rationale": "Simplifies initial deployment; enterprise auth deferred", "alternatives": ["JWT from day 1", "Azure AD B2C"]}
  ],
  "projectStructure": [
    "api/main.py — FastAPI application entry point",
    "api/__init__.py",
    "api/routers/__init__.py",
    "api/routers/rules.py — Rule CRUD endpoints",
    "api/routers/sources.py — Data source CRUD endpoints",
    "api/routers/jobs.py — Job submission and monitoring",
    "api/routers/metrics.py — DQ metrics and scores",
    "api/models/__init__.py",
    "api/models/database.py — SQLAlchemy ORM models",
    "api/schemas/__init__.py",
    "api/schemas/rules.py — Rule Pydantic schemas",
    "api/schemas/sources.py — Source Pydantic schemas",
    "api/schemas/jobs.py — Job Pydantic schemas",
    "api/schemas/metrics.py — Metrics Pydantic schemas",
    "api/config.py — Application configuration",
    "api/dependencies.py — FastAPI dependencies (DB session, etc.)",
    "engine/__init__.py",
    "engine/rule_engine.py — Rule parser, loader, evaluator",
    "engine/dimensions/__init__.py",
    "engine/dimensions/completeness.py",
    "engine/dimensions/uniqueness.py",
    "engine/dimensions/accuracy.py",
    "engine/dimensions/consistency.py",
    "engine/dimensions/timeliness.py",
    "engine/dimensions/validity.py",
    "connectors/__init__.py",
    "connectors/base.py — Abstract DataConnector interface",
    "connectors/adls_gen2.py — ADLS Gen2 implementation",
    "connectors/delta_table.py — Delta Lake implementation",
    "connectors/sql_server.py — SQL Server implementation",
    "spark/__init__.py",
    "spark/dq_job.py — Main DQ check Spark job",
    "spark/submit.py — Job submission utilities",
    "dashboard/package.json",
    "dashboard/tsconfig.json",
    "dashboard/src/App.tsx",
    "dashboard/src/pages/Dashboard.tsx",
    "dashboard/src/pages/Rules.tsx",
    "dashboard/src/pages/History.tsx",
    "dashboard/src/components/ScoreCard.tsx",
    "dashboard/src/components/DimensionChart.tsx",
    "infra/Dockerfile.api",
    "infra/Dockerfile.dashboard",
    "infra/docker-compose.yml",
    "infra/azure/main.bicep",
    "rules/example_rules.yaml",
    "tests/conftest.py",
    "tests/test_rule_engine.py",
    "tests/test_dimensions.py",
    "tests/test_api_rules.py",
    "tests/test_api_sources.py",
    "tests/test_connectors.py"
  ],
  "apiEndpoints": [
    {"method": "GET", "path": "/api/health", "description": "Health check"},
    {"method": "POST", "path": "/api/rules", "description": "Create a DQ rule"},
    {"method": "GET", "path": "/api/rules", "description": "List rules with filtering"},
    {"method": "GET", "path": "/api/rules/{id}", "description": "Get rule by ID"},
    {"method": "PUT", "path": "/api/rules/{id}", "description": "Update a rule"},
    {"method": "DELETE", "path": "/api/rules/{id}", "description": "Delete a rule"},
    {"method": "POST", "path": "/api/rules/import", "description": "Import rules from YAML"},
    {"method": "POST", "path": "/api/sources", "description": "Create data source connection"},
    {"method": "GET", "path": "/api/sources", "description": "List data sources"},
    {"method": "GET", "path": "/api/sources/{id}", "description": "Get source by ID"},
    {"method": "PUT", "path": "/api/sources/{id}", "description": "Update source"},
    {"method": "DELETE", "path": "/api/sources/{id}", "description": "Delete source"},
    {"method": "POST", "path": "/api/sources/{id}/test", "description": "Test source connection"},
    {"method": "POST", "path": "/api/jobs", "description": "Submit DQ job"},
    {"method": "GET", "path": "/api/jobs", "description": "List jobs with filtering"},
    {"method": "GET", "path": "/api/jobs/{id}", "description": "Get job status and details"},
    {"method": "POST", "path": "/api/jobs/{id}/retry", "description": "Retry failed job"},
    {"method": "GET", "path": "/api/metrics/summary", "description": "Overall DQ score summary"},
    {"method": "GET", "path": "/api/metrics/dimensions", "description": "Per-dimension scores"},
    {"method": "GET", "path": "/api/metrics/history", "description": "Historical DQ scores"},
    {"method": "GET", "path": "/api/metrics/sources/{id}", "description": "DQ scores per source"}
  ],
  "databaseSchema": [
    {"table": "rules", "columns": ["id UUID PK", "name VARCHAR(255) NOT NULL", "description TEXT", "dimension VARCHAR(50) NOT NULL", "source_id UUID FK", "config JSONB NOT NULL", "severity VARCHAR(20) DEFAULT 'warning'", "is_active BOOLEAN DEFAULT true", "created_at TIMESTAMP", "updated_at TIMESTAMP"]},
    {"table": "data_sources", "columns": ["id UUID PK", "name VARCHAR(255) NOT NULL", "type VARCHAR(50) NOT NULL", "connection_config JSONB NOT NULL", "is_active BOOLEAN DEFAULT true", "last_tested_at TIMESTAMP", "created_at TIMESTAMP", "updated_at TIMESTAMP"]},
    {"table": "dq_runs", "columns": ["id UUID PK", "source_id UUID FK", "status VARCHAR(20) NOT NULL", "started_at TIMESTAMP", "completed_at TIMESTAMP", "total_rules INT", "passed_rules INT", "failed_rules INT", "error_message TEXT", "parameters JSONB"]},
    {"table": "dq_results", "columns": ["id UUID PK", "run_id UUID FK", "rule_id UUID FK", "dimension VARCHAR(50)", "column_name VARCHAR(255)", "metric_value FLOAT", "threshold FLOAT", "passed BOOLEAN", "details JSONB", "created_at TIMESTAMP"]},
    {"table": "dq_scores", "columns": ["id UUID PK", "run_id UUID FK", "source_id UUID FK", "dimension VARCHAR(50)", "score FLOAT NOT NULL", "total_checks INT", "passed_checks INT", "created_at TIMESTAMP"]},
    {"table": "schedules", "columns": ["id UUID PK", "name VARCHAR(255)", "source_id UUID FK", "cron_expression VARCHAR(100)", "rule_ids JSONB", "is_active BOOLEAN DEFAULT true", "last_run_at TIMESTAMP", "next_run_at TIMESTAMP", "created_at TIMESTAMP"]}
  ],
  "ruleEngineDesign": "The rule engine uses a pipeline pattern: 1) Load rules from YAML/DB → 2) Parse into Rule objects with dimension, target column, config → 3) For each rule, dispatch to the appropriate DimensionCalculator → 4) Calculator receives a DataFrame and rule config, returns a DQResult with metric_value, passed/failed, details. YAML schema uses a 'checks' array where each check specifies dimension, column, operator, threshold. The engine supports batch execution of multiple rules against the same dataset.",
  "sparkJobDesign": "DQ jobs run as PySpark applications. The dq_job.py is the entry point: 1) Receive job config (source, rules) → 2) Create SparkSession → 3) Load data via connector → 4) Execute rules via engine → 5) Collect results → 6) Write results to PostgreSQL → 7) Calculate aggregate scores. For dev/testing, jobs run in local Spark mode. For production, submit to Spark cluster (Synapse/Databricks) via spark-submit or REST API.",
  "deploymentArchitecture": "Azure Container Apps hosts API and Dashboard as separate container apps with shared environment. PostgreSQL uses Azure Database for PostgreSQL Flexible Server. ADLS Gen2 for storing DQ artifacts/reports. Key Vault for secrets. Managed Identity for inter-service auth. Container Apps auto-scaling based on HTTP traffic. Spark jobs submitted to Azure Synapse Spark pools or Databricks via REST API.",
  "nextSteps": [
    "Developer to implement all components following this architecture",
    "Start with database models and API skeleton",
    "Implement engine dimensions in parallel",
    "Scaffold dashboard last"
  ]
}
